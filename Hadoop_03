Hadoop新手学习指导

对于我们新手入门学习hadoop大数据存储的朋友来说，首先了解一下云计算和云计算技术是有必要的。
下面先是介绍云计算和云计算技术的：
云计算，是一种基于互联网的计算方式，通过这种方式，共享的软硬件资源和信息可以按需求提供给计算机和其他设备，主要是基于互联网的相关服务地增加、使用和交付模式，通常涉及通过互联网来提供动态易扩展且经常是虚拟化的资源。
云是网络、互联网的一种比喻说法。过去在图中往往用云来表示电信网，后来也用来表示互联网和底层基础设施的抽象。
狭义云计算指IT基础设施的交付和使用模式，指通过网络以按需、易扩展的方式获得所需资源；广义云计算指服务地交付和使用模式，指通过网络以按需、易扩展的方式获得所需服务。
这种服务可以是IT和软件、互联网相关，也可是其他服务。它意味着计算也可作为一种商品通过互联网进行流通。

什么是云计算？
1) 简介
大家知道什么叫做云计算吗？事实上，目前并没有一个确定的定义。
然而概括来讲，所谓的云计算，指的就是把你的软件和服务统一部署在数据中心，统一管理，从而实现高伸缩性。
云计算拥有以下特点：
(1) 虚拟化和自动化
(2) 服务器，存储介质，网络等资源都可以随时替换
(3) 所有的资源都由云端统一管理
(4) 高度的伸缩性以满足业务需求
(5) 集中于将服务传递给业务
2) 云计算的部署方式
从部署方式来说，总共有两类云计算：
(1)私有云：数据中心部署在企业内部，由企业自行管理。微软为大家提供了Dynamic Data Center Toolkit，来方便大家管理自己的数据中心。
(2) 公共云：数据中心由第三方的云计算供应商提供，供应商帮助企业管理基础设施（例如硬件，网络，等等）。
企业将自己的软件及服务部属在供应商提供的数据中心，并且支付一定的租金。Windows Azure正是这样一个公共云平台。
3) 云计算的运营方式
从运营方式来说，总共有三类云计算：
(1) 软件即服务（SaaS）：云计算运营商直接以服务的形式供应软件，供最终用户使用。
有些服务还提供了SDK，从而使得第三方开发人员可以进行二次开发。
在这种运营模式下，开发人员通常只能针对现有的产品开发插件，而无法充分挖掘平台和操作系统的特点，
不过他们可以在现有产品的基础上添加新的功能，而不必从头开始实现。
微软的Bing，Windows Live，Microsoft Business Productivity Online等产品就属于这一类型。
(2) 平台即服务（PaaS）：云计算运营商将自己的开发及部署平台提供给第三方开发人员，第三方开发人员在这个平台上开发自己的软件和服务，供自己或其它用户使用。在这种运营模式下，开发人员有了更多的自由，可以发挥出平台的强大功能，而不受现有产品的束缚。Windows Azure正是这样一个产品。
(3) 基础设施即服务（IaaS）：云计算运营商提供但不管理基础设施，第三方开发人员将开发好的软件和服务交给自己公司的IT管理员，由IT管理员负责部署及管理。在这种运营模式下，开发人员和IT管理员有最大限度的自由，然而由于必须自行管理部分基础设施，因此成本通常也会较大，对管理员的要求也会较高。目前微软尚未提供IaaS的云计算运营模式，不过我们正在考虑如何给予开发人员和IT管理员更多的自由。
4) 总结
云计算指的就是把你的软件和服务统一部署在数据中心，统一管理，从而实现高伸缩性。
从部署方式来说，云计算可以分为私有云和公共云。从运营方式来说，云计算可以分成SaaS，PaaS，IaaS三类。

什么是云计算技术？
一 首先让大家明白什么是云端，所谓云端需要两层理解
(1) 服务不在本地，这一层可以理解为服务器
(2) 它和普通的服务器是不一样的，这些云端的服务器的资源是共享的，一旦一个服务器不能承受，将会把任务分配给其他机器。

二 云技术与其他技术的区别：
云技术可以使用的语言有java,c++等。
云技术的开发，并没有发展什么新语言，而是在其他语言的基础上。比如Java语言。
与其他技术，最显著的区别，不是在开发上，而是在于架构上，最显著的特点是分布式。

三 下面给大家讲一下较火云技术：
1. Hadoop
Hadoop是一个框架，它是由Java语言来实现的。
Hadoop是处理大数据技术.
Hadoop可以处理云计算产生大数据，需要区分hadoop并不是云计算。它和云计算密不可分。
详细见下面内容。
(1) Hadoop是如何产生的
Hadoop产生是互联网的产物，也是必然。
大家都知道，我们上网时需要服务器的。
假如世界上只有一台电脑，根本不需要服务器。
如果有10台服务器，100台，1000台，上万台，那么我们该如何让大家相互通信，共享知识，所以我们产生了互联网。
互联网产生，全世界都可以通信，知识如此居多，我们像获取更多的知识，想获取新技术，获取新知识，通过什么，国内通过百度，国外也有许多，比如Google。
可是百度和谷歌的用户有多少，多了不说，最起码有上亿的用户。并且这些用户每天上百度，上谷歌，又会产生多少数据，查询多少数据。
那么他们怎么承受如此多用户。这不是一台电脑、一台服务器能完成的事情。
Hadoop就是一个解决方案。
Hadoop是一个分布式方案，能够把压力分摊到其他服务器。
至于如何做到的，可以深入了解Hadoop的maprecude等知识。
想学习hadoop：可以查看下面内容：
Hadoop到底能做什么？怎么用hadoop？
hadoop新手入门视频
零基础学习hadoop到上手工作线路指导
2. Openstack
Openstack是搭建云平台技术，可以搭建公有云，私有云，和混合云。
OpenStack是开源的云管理平台，用来统一管理多个虚拟化集群的框架。
Openstack目前分为两种
(1) openstack的运维
(2) openstack的二次开发
目前来讲，国内真正对Openstack二次开发的很少，这方面的人才也是比较稀缺，网上资料也比较少，淘宝上资料也稀缺，只有很少一部分。
建议向高工资的朋友，可以从这方面下点功夫。
了解openstack可以查看：
一分钟快速入门openstack
全面认识openstack（二）：OpenStack架构详解
openstack文档资料大全苦心搜集
openstack入门视频
3. Cloud Foundry
Cloud Foundry是一个开源的平台即服务产品，它提供给开发者自由度去选择云平台，开发框架和应用服务。
Cloud Foundry最初由 VMware 发起，得到了业界广泛的支持，它使得开发者能够更快更容易的开发，测试，部署和扩展应用。
Cloud Foundry是一个开源项目，用户可以使用多种私有云发行版，也可以使用公共云服务。
详细可查看
什么是Cloud Foundry，该如何入门
4. nosql
nosql即not only sql。
nosql数据库是一种比较低级的数据库，关系型数据库是由nosql数据库发展而来。
什么是关系型数据库，这里不从概念上区别，常用的SqlServer，mysql,oracle都是关系型数据库。
关系型数据库顾名思义，数据库关系明确严谨。
而nosql则是一种数据关系不严谨的数据库。一个key和value。
详细可查看
什么是nosql，nosql为什么会兴起,nosq有哪些主流数据库
nosql资料文档分享（1）
nosql资料文档分享（2）

在世界上云计算已经大面流行，有很流行的Google Drive、SkyDrive、Dropbox、亚马逊云服务等等。在国内百度云存储、360云存储都是比较流行的。
我们就应该会想到大数据存储，目前开源市场上最流行的应该是hadoop分布式存储，
已经有大部分互联网公司已经开始使用，例如百度、360、阿里巴巴，
其中一部分公司已经把hadoop作为他们的核心产品例如英特尔、IBM并为部分工作提供过大数据的解决方案，大家可以了解一下英特尔在不行业提供的解决方案：

面向智能交通的大数据和英特尔® 智能系统解决方案
物联网商机和技术挑战(英特尔)
大数据在医疗行业的应用
英特尔IT开源混合云

Hadoop 是一个能够对大量数据进行分布式处理的软件框架，它是一种技术的实现，是云计算技术中重要的组成部分，
云计算的概念更广泛且偏向业务而不是必须拘泥于某项具体技术，
云计算的存在只是一种新的商业计算模型和服务模式。
因此，云计算才会出现“横看成岭侧成峰，远近高低各不同”，各种各样层出不穷的理解。
Hadoop 大数据以后的方向：超越Hadoop的大数据未来的研究方向

所以hadoop在大数据方面以后是主流，对我们想接触大数据的朋友是有必要学习hadoop的，对于初学的朋友：
Hadoop前景、毕业薪酬，你所关心的 我想这些更是大家想要关心的内容，也是初学朋友有必要的看的。

对于初学Hadoop的朋友来说可能基于迫切寻找一本入门的书，
我个人觉得不用于急于寻找书，先了解Hadoop是否做什么、它能做什么、能带来什么 Hadoop使用场景、Hadoop到底能做什么？怎么用Hadoop？，
当大家对这些有所了解，就会如何入手学习Hadoop

接下来大家应该进行系统性的学习Hadoop了，
我个人建议不要盲目的去搭建Hadoop环境，熟悉了解Hadoop基本知识及其所需要的知识例如java基础、linux环境、linux常用命令，它相关产品及其衍生产品，他们之间是什么关系如何工作，每个产品它们的特点是什么，
下面是Hadoop一些基本知识：

Hadoop HDFS文件系统的特征
(1) 存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中。支持很大单个文件。
(2) 提供数据的高可靠性，单个或者多个节点不工作，对系统不会造成任何影响，数据仍然可用。
(3) 提供对这些信息的快速访问，并提供可扩展的方式。能够通过简单加入更多服务器的方式就能够服务更多的客户端。
(4) HDFS是针对MapReduce设计的，使得数据尽可能根据其本地局部性进行访问与计算。

Hadoop简介(1):什么是Map/Reduce
# Hadoop简介
Hadoop就是一个实现了Google云计算系统的开源系统，包括并行计算模型Map/Reduce，分布式文件系统HDFS，以及分布式数据库Hbase，
同时Hadoop的相关项目也很丰富，包括ZooKeeper，Pig，Chukwa，Hive，Hbase，Mahout，flume等.
这里详细分解这里面的概念让大家通过这篇文章了解到底是什么hadoop：
# 什么是Map/Reduce
看下面的各种解释：
(1)MapReduce是hadoop的核心组件之一，
hadoop要分布式包括两部分，一是分布式文件系统hdfs,一部是分布式计算框，就是mapreduce,缺一不可，
也就是说，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程。
(2)Mapreduce是一种编程模型，是一种编程方法，抽象理论。
(3)下面是一个关于一个程序员是如何个妻子讲解什么是MapReduce？文章很长请耐心的看。
我问妻子：“你真的想要弄懂什么是MapReduce？” 她很坚定的回答说“是的”。 因此我问道：
我： 你是如何准备洋葱辣椒酱的？（以下并非准确食谱，请勿在家尝试）
妻子： 我会取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机里研磨。这样就能得到洋葱辣椒酱了。
妻子： 但这和MapReduce有什么关系？
我： 你等一下。让我来编一个完整的情节，这样你肯定可以在15分钟内弄懂MapReduce.
妻子： 好吧。
我：现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？
妻子： 我会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。
我： 没错，让我们把MapReduce的概念应用到食谱上。Map和Reduce其实是两种操作，我来给你详细讲解下。
Map（映射）:
把洋葱、番茄、辣椒和大蒜切碎，是各自作用在这些物体上的一个Map操作。
所以你给Map一个洋葱，Map就会把洋葱切碎。
同样的，你把辣椒，大蒜和番茄一一地拿给Map，你也会得到各种碎块。
所以，当你在切像洋葱这样的蔬菜时，你执行就是一个Map操作。
Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。
在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把坏洋葱丢了就行了。
所以，如果出现坏洋葱了，Map操作就会过滤掉坏洋葱而不会生产出任何的坏洋葱块。
Reduce（化简）:
在这一阶段，你将各种蔬菜碎都放入研磨机里进行研磨，你就可以得到一瓶辣椒酱了。
这意味要制成一瓶辣椒酱，你得研磨所有的原料。
因此，研磨机通常将map操作的蔬菜碎聚集在了一起。
妻子： 所以，这就是MapReduce?
我： 你可以说是，也可以说不是。 其实这只是MapReduce的一部分，MapReduce的强大在于分布式计算。
妻子： 分布式计算？ 那是什么？请给我解释下吧。
我： 没问题。
我： 假设你参加了一个辣椒酱比赛并且你的食谱赢得了最佳辣椒酱奖。
得奖之后，辣椒酱食谱大受欢迎，于是你想要开始出售自制品牌的辣椒酱。
假设你每天需要生产10000瓶辣椒酱，你会怎么办呢？
妻子： 我会找一个能为我大量提供原料的供应商。
我：是的..就是那样的。那你能否独自完成制作呢？也就是说，独自将原料都切碎？
仅仅一部研磨机又是否能满足需要？而且现在，我们还需要供应不同种类的辣椒酱，像洋葱辣椒酱、青椒辣椒酱、番茄辣椒酱等等。
妻子： 当然不能了，我会雇佣更多的工人来切蔬菜。我还需要更多的研磨机，这样我就可以更快地生产辣椒酱了。
我：没错，所以现在你就不得不分配工作了，你将需要几个人一起切蔬菜。每个人都要处理满满一袋的蔬菜，而每一个人都相当于在执行一个简单的Map操作。
每一个人都将不断的从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。
这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块、和蒜蓉等等。
妻子：但是我怎么会制造出不同种类的番茄酱呢？
我：现在你会看到MapReduce遗漏的阶段—搅拌阶段。MapReduce将所有输出的蔬菜碎都搅拌在了一起，这些蔬菜碎都是在以key为基础的 map操作下产生的。
搅拌将自动完成，你可以假设key是一种原料的名字，就像洋葱一样。
所以全部的洋葱keys都会搅拌在一起，并转移到研磨洋葱的研磨器里。
这样，你就能得到洋葱辣椒酱了。
同样地，所有的番茄也会被转移到标记着番茄的研磨器里，并制造出番茄辣椒酱。
（4）上面都是从理论上来说明什么是MapReduce，那么咱们在MapReduce产生的过程和代码的角度来理解这个问题。
如果想统计下过去10年计算机论文出现最多的几个单词，看看大家都在研究些什么，那收集好论文后，该怎么办呢？
方法一：
我可以写一个小程序，把所有论文按顺序遍历一遍，统计每一个遇到的单词的出现次数，最后就可以知道哪几个单词最热门了。
这种方法在数据集比较小时，是非常有效的，而且实现最简单，用来解决这个问题很合适。
方法二：
写一个多线程程序，并发遍历论文。
这个问题理论上是可以高度并发的，因为统计一个文件时不会影响统计另一个文件。
当我们的机器是多核或者多处理器，方法二肯定比方法一高效。
但是写一个多线程程序要比方法一困难多了，我们必须自己同步共享数据，比如要防止两个线程重复统计文件。
方法三：
把作业交给多个计算机去完成。
我们可以使用方法一的程序，部署到N台机器上去，然后把论文集分成N份，一台机器跑一个作业。
这个方法跑得足够快，
但是部署起来很麻烦，我们要人工把程序copy到别的机器，要人工把论文集分开，最痛苦的是还要把N个运行结果进行整合（当然我们也可以再写一个程序）。
方法四：
让MapReduce来帮帮我们吧！
MapReduce本质上就是方法三，但是如何拆分文件集，如何copy程序，如何整合结果这些都是框架定义好的。
我们只要定义好这个任务（用户程序），其它都交给MapReduce。
# map函数和reduce函数　　
map函数和reduce函数是交给用户实现的，这两个函数定义了任务本身。
(1) map函数：
接受一个键值对（key-value pair），产生一组中间键值对。
MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。
(2) reduce函数：
接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。
# 统计词频的MapReduce函数的核心代码非常简短，主要就是实现这两个函数。
　　map(String key, String value):
　　// key: document name
　　// value: document contents

　　for each word w in value:
　　EmitIntermediate(w, "1");
　　reduce(String key, Iterator values):
　　// key: a word
　　// values: a list of counts

　　int result = 0;
　　for each v in values:
　　result += ParseInt(v);
　　Emit(AsString(result));
在统计词频的例子里，
map函数接受的键是文件名，值是文件的内容，
map逐个遍历单词，每遇到一个单词w，就产生一个中间键值对<w, "1">，
这表示单词w咱又找到了一个；
MapReduce将键相同（都是单词w）的键值对传给reduce函数，
这样reduce函数接受的键就是单词w，值是一串"1"（最基本的实现是这样，但可以优化），个数等于键为w的键值对的个数，
然后将这些“1”累加就得到单词w的出现次数。
最后这些单词的出现次数会被写到用户定义的位置，存储在底层的分布式存储系统（GFS或HDFS）。
# 工作原理
上图是论文里给出的流程图。
一切都是从最上方的user program开始的，user program链接了MapReduce库，实现了最基本的Map函数和Reduce函数。
图中执行的顺序都用数字标记了。
1. MapReduce库先把user program的输入文件划分为M份（M为用户定义），每一份通常有16MB到64MB，如图左方所示分成了split0~4；
然后使用fork将用户进程拷贝到集群内其它机器上。
2. user program的副本中有一个称为master，其余称为worker，
master是负责调度的，为空闲worker分配作业（Map作业或者Reduce作业），
worker的数量也是可以由用户指定的。
3. 被分配了Map作业的worker，开始读取对应分片的输入数据，
Map作业数量是由M决定的，和split一一对应；
Map作业从输入数据中抽取出键值对，每一个键值对都作为参数传递给map函数，
map函数产生的中间键值对被缓存在内存中。
4. 缓存的中间键值对会被定期写入本地磁盘，而且被分为R个区，R的大小是由用户定义的，将来每个区会对应一个Reduce作业；
这些中间键值对的位置会被通报给master，master负责将信息转发给Reduce worker。
5. master通知分配了Reduce作业的worker它负责的分区在什么位置（肯定不止一个地方，每个Map作业产生的中间键值对都可能映射到所有R个不同分区），
当Reduce worker把所有它负责的中间键值对都读过来后，先对它们进行排序，使得相同键的键值对聚集在一起。
因为不同的键可能会映射到同一个分区也就是同一个Reduce作业（谁让分区少呢），所以排序是必须的。
6. reduce worker遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给reduce函数，reduce函数产生的输出会添加到这个分区的输出文件中。
7. 当所有的Map和Reduce作业都完成了，master唤醒正版的user program，MapReduce函数调用返回user program的代码。
8. 所有执行完毕后，MapReduce输出放在了R个分区的输出文件中（分别对应一个Reduce作业）。
用户通常并不需要合并这R个文件，而是将其作为输入交给另一个MapReduce程序处理。
整个过程中，输入数据是来自底层分布式文件系统（GFS）的，中间数据是放在本地文件系统的，最终输出数据是写入底层分布式文件系统（GFS）的。
9. 而且我们要注意Map/Reduce作业和map/reduce函数的区别：
Map作业处理一个输入数据的分片，可能需要调用多次map函数来处理每个输入键值对；
Reduce作业处理一个分区的中间键值对，期间要对每个不同的键调用一次reduce函数，Reduce作业最终也对应一个输出文件。
# 总结：
通过以上你是否了解什么是MapReduce了那，什么是key, 怎么过滤有效数据，怎么得到自己想要的数据。
MapReduce是一种编程思想，可以使用java来实现，C++来实现。
Map的作用是过滤一些原始数据，Reduce则是处理这些数据，得到我们想要的结果，比如你想造出番茄辣椒酱。
也就是我们使用hadoop，比方来进行日志处理之后，得到我们想要的关心的数据

Mapreduce 整个工作机制图
图中1：表示待处理数据，比如日志，比如单词计数
图中2：表示map阶段，对他们split，然后送到不同分区
图中3：表示reduce阶段，对这些数据整合处理。
图中4：表示二次mapreduce,这个是mapreduce的链式，

Hadoop mapper类的阅读
在Hadoop的mapper类中，有4个主要的函数，分别是：setup，clearup，map，run。
代码如下：
protected void setup(Context context) throws IOException, InterruptedException {
// NOTHING
}

protected void map(KEYIN key, VALUEIN value, Context context) throws IOException, InterruptedException {
  context.write((KEYOUT) key, (VALUEOUT) value);
}

protected void cleanup(Context context) throws IOException, InterruptedException {
// NOTHING
}

public void run(Context context) throws IOException, InterruptedException {
    setup(context);
    while (context.nextKeyValue()) {
      map(context.getCurrentKey(), context.getCurrentValue(), context);
    }
    cleanup(context);
  }
}
由上面的代码，我们可以了解到，当调用到map时，通常会先执行一个setup函数，最后会执行一个cleanup函数。
而默认情况下，这两个函数的内容都是nothing。因此，当map方法不符合应用要求时，可以试着通过增加setup和cleanup的内容来满足应用的需求。


Hadoop reducer类的阅读
在Hadoop的reducer类中，有3个主要的函数，分别是：setup，clearup，reduce。代码如下：
/**
* Called once at the start of the task.
*/
protected void setup(Context context) throws IOException, InterruptedException {
  // NOTHING
}

/**
* This method is called once for each key. Most applications will define
* their reduce class by overriding this method. The default implementation
* is an identity function.
*/
@SuppressWarnings("unchecked")
protected void reduce(KEYIN key, Iterable<VALUEIN> values, Context context) throws IOException, InterruptedException {
  for(VALUEIN value: values) {
    context.write((KEYOUT) key, (VALUEOUT) value);
  }
}

/**
 * Called once at the end of the task.
 */
protected void cleanup(Context context) throws IOException, InterruptedException {
  // NOTHING
}

在用户的应用程序中调用到reducer时，会直接调用reducer里面的run函数，
其代码如下：
/**
 * control how the reduce task works.
 */
 @SuppressWarnings("unchecked")
 public void run(Context context) throws IOException, InterruptedException {
  setup(context);
  while (context.nextKey()) {
    reduce(context.getCurrentKey(), context.getValues(), context);
    // If a back up store is used, reset it
    ((ReduceContext.ValueIterator)(context.getValues().iterator())).resetBackupStore();
  }
  cleanup(context);
 }
由上面的代码，我们可以了解到，当调用到reduce时，通常会先执行一个setup函数，最后会执行一个cleanup函数。
而默认情况下，这两个函数的内容都是nothing。因此，当reduce不符合应用要求时，可以试着通过增加setup和cleanup的内容来满足应用的需求。

Mapreduce shuffle和排序

上面这些都是Hadoop核心部分，当这些有所了解后，大家基本上可以具备大家hadoop环境的条件了。

Hadoop部署方式为单机模式、伪分布式、完全分布式。
对单机模式大家可以不用去关心和学习，
在学习中我个人建议是搭建伪分布式，
完全分布式是生产环境中使用，
当大家把伪分布式后，必须对完全分布式有所了解，知道是如何工作的，也可以试着搭建hadoop的完成分布式。

现在Hadoop已经发行了最新的2.2.x版本，但是不测试不够全面不够稳定，大家应该选择比较稳定的版本学习，因为在公司中还是会使用稳定的版本，
2.2.x版本中一些处理机制和方案是值得我们学习的，需要所有了解， Hadoop 各个发布版的特性以及稳定性

下面是搭建Hadoop的安装步骤。
搭建伪分布式：Hadoop 伪分布式搭建
完全分布式：Hadoop 三节点集群安装配置详细实例

大家安装完成后需要一些基本的练级:
Hadoop shell命令介绍

大家这些有了基础性的学习后，这时候是比较适合找本书来系统性的学习Hadoop。
Hadoop权威指南(第2版)
Hadoop相关文档下载
about云资源汇总V1.2 在这里可以下载到大家所有需要学习的相关资料

有一点想提醒初学的朋友，在学习Hadoop开发的时候不要使用Hadoop eclipse插件，这样会给你带来不必要的问题，
你可以在eclipse使用maven工具下载Hadoop资源包，然后写好mapreduce代码打包后传上自己的服务，使用命令启动运行。

到这里大家其实已经对Hadoop有了系统性的认识和学习，我想后面的学习每个人的学习方式都是不同的，大家所需要的资料问题在论坛上都可以找的到。
祝大家学习Hadoop愉快。

还有篇文章不得不看，从另外角度阐述该如何学习Hadoop，可以查看零基础学习hadoop到上手工作线路指导
